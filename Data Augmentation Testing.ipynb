{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os, cv2\n",
    "\n",
    "TRAIN_DIR = 'C:/Users/Brendon/URE18/images'\n",
    "TEST_DIR = 'C:/Users/Brendon/URE18/Test_Images'\n",
    "TRAIN_LABEL_DIR = 'Data_Entry_2017.csv'\n",
    "AUG_DATA_DIR = 'Augmented Data/'\n",
    "IMG_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Training Data & Label Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 4999/4999 [04:54<00:00, 16.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4999, 256, 256, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_batch(directory):\n",
    "    img_array = []\n",
    "    for img in tqdm(os.listdir(directory)):\n",
    "        path = os.path.join(directory, img)\n",
    "        img = cv2.resize(cv2.imread(path, cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE))\n",
    "        img_array.append(img)\n",
    "    return img_array\n",
    "\n",
    "training_img = create_batch(TRAIN_DIR)\n",
    "training_img = np.array(training_img)\n",
    "training_img = np.expand_dims(training_img, axis = 3)\n",
    "\n",
    "training_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4999, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode for Multiple Labels\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "read_data = pd.read_csv(TRAIN_LABEL_DIR)\n",
    "split_labels = [items.split('|') for items in read_data.Finding_Labels]\n",
    "one_hot = MultiLabelBinarizer()\n",
    "training_labels = one_hot.fit_transform(split_labels)\n",
    "training_labels = training_labels[:4999]\n",
    "\n",
    "training_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = image.ImageDataGenerator(\n",
    "                rotation_range=40,\n",
    "                width_shift_range=0.2,\n",
    "                height_shift_range=0.2,\n",
    "                shear_range=0.2,\n",
    "                zoom_range=0.2,\n",
    "                fill_mode='nearest')\n",
    "\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f11362c56f0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdatagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_to_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAUG_DATA_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_prefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'aug_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mbatches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatches\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    print('Epoch: ', e)\n",
    "    batches = 0\n",
    "    #for x_batch, y_batch in datagen.flow(training_img, training_labels, batch_size=32, save_to_dir = AUG_DATA_DIR, save_prefix = 'aug_' + str(batches)):\n",
    "        #model.fit(x_batch, y_batch)\n",
    "        batches += 1\n",
    "        if batches >= 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/Load Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_name):\n",
    "    model_str = str(input(\"Save model as: \"))\n",
    "\n",
    "    # serialize model to JSON\n",
    "    model_name = model.to_json()\n",
    "    with open(\"CNN Models/\" + model_str + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_name)\n",
    "\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"CNN Models/\" + model_str + \".h5\")\n",
    "    print(\"Saved \" + model_str + \" and weights to CNN Models folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model_str = str(input(\"Name of model to load: \"))\n",
    "\n",
    "    # load json and create model\n",
    "    json_file = open('CNN Models/' + model_str + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"CNN Models/\" + model_str + \".h5\")\n",
    "    print(\"Loaded \" + model_str + \" and weights from CNN Models folder\")\n",
    "    \n",
    "    return loaded_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
