{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11212, 256, 256)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "test_img = pickle.load(open('pickle_test_img.p', 'rb'))\n",
    "test_img = np.squeeze(test_img, axis = -1)\n",
    "test_img.shape\n",
    "\n",
    "#training_img_1 = pickle.load(open('processed_train_img_1.p', 'rb'))\n",
    "#training_img_2 = pickle.load(open('processed_train_img_2.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11212, 65536)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.set_printoptions(threshold=np.nan, linewidth=115)\n",
    "\n",
    "#training_img = np.append(training_img_1, training_img_2, axis = 0)\n",
    "\n",
    "reshape_array = np.reshape(test_img, (11212, 256 * 256))\n",
    "reshape_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11212, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = pickle.load(open('test_labels.p', 'rb'))\n",
    "test_labels = test_labels.astype(dtype='uint8')\n",
    "test_labels.shape\n",
    "\n",
    "#training_labels = pickle.load(open('training_labels.p', 'rb'))\n",
    "#training_labels = training_labels.astype(dtype='uint8')\n",
    "\n",
    "#print(training_labels[:10])\n",
    "#training_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11212,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Infiltration_test_labels = test_labels[:, 8]\n",
    "Infiltration_test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Infiltration_test_dict = {0: 1745, 1: 1745}\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "RUS = RandomUnderSampler(ratio = Infiltration_test_dict)\n",
    "\n",
    "test_img_res, test_label_res = RUS.fit_sample(reshape_array, Infiltration_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3490,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_int_training_labels = np.array(test_label_res, dtype='uint8')\n",
    "\n",
    "vec_int_training_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_new = test_img_res.reshape((len(vec_int_training_labels), 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3490, 256, 256, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img = np.expand_dims(test_img_new, axis = 3)\n",
    "test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Processed Images\n",
    "pickle.dump(test_img, open('1vAll_test_img.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vec_int_training_labels, open('1vAll_test_labels.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Processed Images"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config = config)\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os, cv2\n",
    "import glob\n",
    "\n",
    "TRAIN_DIR = 'D:/URE18/Images/*.png'\n",
    "TEST_DIR = 'D:/URE18/Test Images/*.png'\n",
    "TRAIN_LABEL_DIR = 'Data_Entry_2017.csv'\n",
    "IMG_SIZE = 256\n",
    "\n",
    "# Save baseline model architecture and weights\n",
    "def save_base(model_name):\n",
    "    model_str = str(input(\"Save model as: \"))\n",
    "\n",
    "    # serialize model to JSON\n",
    "    model_name = baseline.to_json()\n",
    "    with open(\"CNN Models/\" + model_str + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_name)\n",
    "\n",
    "    # serialize weights to HDF5\n",
    "    baseline.save_weights(\"CNN Models/\" + model_str + \".h5\")\n",
    "    print(\"Saved \" + model_str + \" and weights to CNN Models folder\")\n",
    "    \n",
    "# Save Comparison model\n",
    "def save_model(model_name):\n",
    "    model_str = str(input(\"Save model as: \"))\n",
    "\n",
    "    # serialize model to JSON\n",
    "    model_name = model.to_json()\n",
    "    with open(\"CNN Models/\" + model_str + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_name)\n",
    "\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"CNN Models/\" + model_str + \".h5\")\n",
    "    print(\"Saved \" + model_str + \" and weights to CNN Models folder\")\n",
    "    \n",
    "# Load model architecture and weights NOTE: must compile again\n",
    "def load_model():\n",
    "    model_str = str(input(\"Name of model to load: \"))\n",
    "\n",
    "    # load json and create model\n",
    "    json_file = open('CNN Models/' + model_str + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"CNN Models/\" + model_str + \".h5\")\n",
    "    print(\"Loaded \" + model_str + \" and weights from CNN Models folder\")\n",
    "    \n",
    "    return loaded_model\n",
    "\n",
    "# Write history object to a file using pickle\n",
    "def save_history(model_name):\n",
    "    hist_str = str(input(\"Save history as: \"))\n",
    "\n",
    "    pickle.dump(model_name.history, open('Training Histories/'+ hist_str + '.p', 'wb'))\n",
    "    \n",
    "    print(\"Saved \" + hist_str + \" to Training Histories folder\")\n",
    "    \n",
    "# Load history object\n",
    "def load_history():\n",
    "    hist_str = str(input(\"Name of history to load: \"))\n",
    "\n",
    "    loaded_history = pickle.load(open('Training Histories/' + hist_str + '.p', 'rb'))\n",
    "    \n",
    "    print(\"Loaded \" + hist_str + \" from Training Histories folder\")\n",
    "    \n",
    "    return loaded_history\n",
    "\n",
    "half_dict = {'000000000010000': 9000}\n",
    "\n",
    "# Training Data Dictionary for Random Under Sampling and Class Weights\n",
    "\n",
    "test_dict= {'100000000000000': 722,     #Atelectasis---------- 10544 total      3790 single\n",
    "            '010000000000000': 722,     #Cardiomegaly--------- 2452  total      972 single\n",
    "            '001000000000000': 722,     #Consolidation-------- 4224  total      1188 single\n",
    "            '000100000000000': 588,     #Edema---------------- 2140  total      588 single\n",
    "            '000010000000000': 722,     #Effusion------------- 12287 total      3617 single\n",
    "            '000001000000000': 722,     #Emphysema------------ 2263  total      783 single\n",
    "            '000000100000000': 555,     #Fibrosis------------- 1374  total      555 single\n",
    "            '000000010000000': 88,      #Hernia--------------- 191   total      88 single\n",
    "            '000000001000000': 8666,    #Infiltration--------- 18149 total      8666 single\n",
    "            '000000000100000': 722,     #Mass----------------- 5420  total      2004 single\n",
    "            '000000000010000': 722,     #No Finding----------- 53911 total   &   single  \n",
    "            '000000000001000': 722,     #Nodule--------------- 5847  total      2478 single\n",
    "            '000000000000100': 722,     #Pleural_Thickening--- 3021  total      983 single\n",
    "            '000000000000010': 283,     #Pneumonia------------ 1289  total      283 single\n",
    "            '000000000000001': 722 }    #Pneumothorax--------- 4815  total      1942 single\n",
    "\n",
    "def preprocess_img(directory):\n",
    "    img_array = []\n",
    "    # Sort the files that are read and go in order -Ellis Chang\n",
    "    for file in tqdm(sorted(glob.glob(directory))):\n",
    "        img = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        img_array.append(img)\n",
    "    return img_array\n",
    "\n",
    "def show_img(array1, array2, array3, index):\n",
    "    plt.imshow(array1[index], cmap = 'gray')\n",
    "    plt.title(\"{} \\n {}\".format(array2[index], array3[index]))\n",
    "    plt.show\n",
    "    \n",
    "read_data = pd.read_csv(TRAIN_LABEL_DIR)\n",
    "\n",
    "image_num = []\n",
    "for item in read_data.Image_Index:\n",
    "    image_num.append(item)\n",
    "\n",
    "train_num = image_num[11212:]\n",
    "test_num = image_num[:11212]\n",
    "\n",
    "training_img = preprocess_img(TRAIN_DIR)\n",
    "\n",
    "training_img = np.array(training_img)\n",
    "\n",
    "###### Save Processed Images\n",
    "pickle.dump(training_img[:int(len(training_img)/2)], open('processed_train_img_1.p', 'wb'))\n",
    "\n",
    "pickle.dump(training_img[int(len(training_img)/2):], open('processed_train_img_2.p', 'wb'))\n",
    "\n",
    "raw_labels = []\n",
    "for label in read_data.Finding_Labels:\n",
    "    raw_labels.append(label)\n",
    "\n",
    "raw_train_labels = raw_labels[11212:]\n",
    "\n",
    "split_labels = [items.split('|') for items in raw_train_labels]\n",
    "print(split_labels)\n",
    "\n",
    "stacked_labels = np.hstack(split_labels)\n",
    "stacked_labels = np.sort(stacked_labels)\n",
    "\n",
    "Counter(stacked_labels)\n",
    "\n",
    "normalized_dict_1 = {'100000000000000': 1910,     #Atelectasis---------- 10544 total      3790  single\n",
    "                     '000010000000000': 1845,     #Effusion------------- 12287 total      3617  single\n",
    "                     '000000000010000': 2268,     #No Finding----------- 53911 total   &  53911 single  \n",
    "                     '000000000001000': 1271}     #Nodule--------------- 5847  total      2478  single\n",
    "\n",
    "normalized_dict_2 = {'100000000000000': 358,     #Atelectasis---------- 10544 total      3790  single\n",
    "                     '000010000000000': 423,      #Effusion------------- 12287 total      3617  single\n",
    "                     '000000000010000': 0,        #No Finding----------- 53911 total   &  53911 single  \n",
    "                     '000000000001000': 997}      #Nodule--------------- 5847  total      2478  single\n",
    "\n",
    "def stringify(_list):\n",
    "    label_str = \"\"\n",
    "    for integer in _list:\n",
    "        label_str += str(integer)\n",
    "    return label_str\n",
    "\n",
    "str_labels = [stringify(label) for label in training_labels]\n",
    "\n",
    "def vectorize(_str):\n",
    "    arr = []\n",
    "    for letter in _str:\n",
    "        arr.append(int(letter))\n",
    "    return arr\n",
    "\n",
    "vec_training_labels = [vectorize(label) for label in training_label_res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with pickle instead of processing images again\n",
    "training_img_1 = pickle.load(open('training_img_1.p', 'rb'))\n",
    "training_img_2 = pickle.load(open('training_img_2.p', 'rb'))\n",
    "training_img = np.append(training_img_1, training_img_2, axis = 0)\n",
    "\n",
    "test_img = pickle.load(open('pickle_test_img.p', 'rb'))\n",
    "\n",
    "#training_labels = pickle.load(open('training_labels.p', 'rb'))\n",
    "#test_labels = pickle.load(open('test_labels.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import models, optimizers, layers\n",
    "from keras.models import model_from_json\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = models.Sequential()\n",
    "baseline.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "baseline.add(layers.MaxPooling2D((2, 2)))\n",
    "baseline.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "baseline.add(layers.MaxPooling2D((2, 2)))\n",
    "baseline.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "baseline.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "baseline.add(layers.Flatten())\n",
    "baseline.add(layers.Dense(64, activation='relu'))\n",
    "baseline.add(layers.Dense(15, activation='sigmoid'))\n",
    "\n",
    "#baseline.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline.compile(optimizer = optimizers.RMSprop(lr = 1e-5), loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_history = baseline.fit(training_img, training_labels, epochs = 40, validation_split = (1 / 9), batch_size = 128, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_history(base_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_base(base_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_test = baseline.evaluate(test_img, test_labels)\n",
    "print(baseline_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = base_history.history['acc']\n",
    "val_acc = base_history.history['val_acc']\n",
    "loss = base_history.history['loss']\n",
    "val_loss = base_history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(15, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = optimizers.RMSprop(lr = 1e-5), loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4C_4MP_1D = model.fit(training_img, training_labels, epochs = 20, validation_split = (1 / 9), batch_size = 128, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = model.evaluate(test_img, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_history(model_4C_4MP_1D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Results Compared To Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_acc = bases_loaded.history['val_acc']\n",
    "model_acc = model_4C_4MP_1D.history['val_acc']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, base_acc, 'b', label='Base Acc')\n",
    "plt.plot(epochs, model_acc, 'r', label='Model Acc')\n",
    "plt.title('Comparison of Validation Accuracies')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loss = bases_loaded.history['val_loss']\n",
    "model_loss = model_4C_4MP_1D.history['val_loss']\n",
    "plt.plot(epochs, base_loss, 'b', label='Base loss')\n",
    "plt.plot(epochs, model_loss, 'r', label='Model loss')\n",
    "plt.title('Comparison of Validation Losses')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data = pd.read_csv(TRAIN_LABEL_DIR)\n",
    "split_labels = [items.split('|') for items in read_data.Finding_Labels]\n",
    "\n",
    "all_img = np.append(test_img, training_img, axis = 0)\n",
    "print(all_img.shape)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#numpy.set_printoptions(threshold=numpy.nan)\n",
    "str_array = np.hstack(split_labels)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(str_array)\n",
    "#print(integer_encoded)\n",
    "onehot_encoder = OneHotEncoder(sparse = False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded),1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "#print(onehot_encoded[:10])\n",
    "\n",
    "onehot_encoded = onehot_encoded.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(onehot_encoded.shape)\n",
    "print(onehot_encoded.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(onehot_encoded, 0)\n",
    "print(onehot_encoded.ndim)\n",
    "print(onehot_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(str_array))\n",
    "print(len(onehot_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_data = []\n",
    "index = 0\n",
    "for i in range(0, len(all_img)):\n",
    "    for label in split_labels[i]:\n",
    "        flattened_data.append((onehot_encoded[index], all_img[i]))\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold = np.nan, linewidth = 115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(flattened_data[1])\n",
    "final_img = []\n",
    "\n",
    "i = 0\n",
    "while i < len(flattened_data):\n",
    "    final_img.append(flattened_data[i][1])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_img = np.array(final_img[:(len(final_img)/3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flattened_data[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(flattened_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
