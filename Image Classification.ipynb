{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.image as img\n",
    "from tqdm import tqdm\n",
    "import os, cv2\n",
    "\n",
    "TRAIN_DIR = 'C:\\\\Users\\\\rocke\\\\URE18\\\\Images'\n",
    "TEST_DIR = 'C:\\\\Users\\\\rocke\\\\URE18\\\\Test Images'\n",
    "TRAIN_LABEL_DIR = 'Data_Entry_2017.csv'\n",
    "IMG_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save baseline model architecture and weights\n",
    "def save_base(model_name):\n",
    "    model_str = str(input(\"Save model as: \"))\n",
    "\n",
    "    # serialize model to JSON\n",
    "    model_name = baseline.to_json()\n",
    "    with open(\"CNN Models/\" + model_str + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_name)\n",
    "\n",
    "    # serialize weights to HDF5\n",
    "    baseline.save_weights(\"CNN Models/\" + model_str + \".h5\")\n",
    "    print(\"Saved \" + model_str + \" and weights to CNN Models folder\")\n",
    "    \n",
    "# Save Comparison model\n",
    "def save_model(model_name):\n",
    "    model_str = str(input(\"Save model as: \"))\n",
    "\n",
    "    # serialize model to JSON\n",
    "    model_name = model.to_json()\n",
    "    with open(\"CNN Models/\" + model_str + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_name)\n",
    "\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"CNN Models/\" + model_str + \".h5\")\n",
    "    print(\"Saved \" + model_str + \" and weights to CNN Models folder\")\n",
    "    \n",
    "# Load model architecture and weights NOTE: must compile again\n",
    "def load_model():\n",
    "    model_str = str(input(\"Name of model to load: \"))\n",
    "\n",
    "    # load json and create model\n",
    "    json_file = open('CNN Models/' + model_str + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"CNN Models/\" + model_str + \".h5\")\n",
    "    print(\"Loaded \" + model_str + \" and weights from CNN Models folder\")\n",
    "    \n",
    "    return loaded_model\n",
    "\n",
    "# Write history object to a file using pickle\n",
    "def save_history(model_name):\n",
    "    hist_str = str(input(\"Save history as: \"))\n",
    "\n",
    "    pickle.dump(model_name.history, open('Training Histories/'+ hist_str + '.p', 'wb'))\n",
    "    \n",
    "    print(\"Saved \" + hist_str + \" to Training Histories folder\")\n",
    "    \n",
    "# Load history object\n",
    "def load_history():\n",
    "    hist_str = str(input(\"Name of history to load: \"))\n",
    "\n",
    "    loaded_history = pickle.load(open('Training Histories/' + hist_str + '.p', 'rb'))\n",
    "    \n",
    "    print(\"Loaded \" + hist_str + \" from Training Histories folder\")\n",
    "    \n",
    "    return loaded_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encode for Multiple Labels\n",
    "'''\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "read_data = pd.read_csv(TRAIN_LABEL_DIR)\n",
    "split_labels = [items.split('|') for items in read_data.Finding_Labels]\n",
    "one_hot = MultiLabelBinarizer()\n",
    "onehot_labels = one_hot.fit_transform(split_labels)\n",
    "test_labels = onehot_labels[:11212]\n",
    "training_labels = onehot_labels[11212:]\n",
    "\n",
    "pickle.dump(training_labels, open('training_labels.p', 'wb'))\n",
    "pickle.dump(test_labels, open('test_labels.p', 'wb'))\n",
    "\n",
    "def create_batch(directory):\n",
    "    img_array = []\n",
    "    for img in tqdm(os.listdir(directory)):\n",
    "        path = os.path.join(directory, img)\n",
    "        img = cv2.resize(cv2.imread(path, cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE))\n",
    "        img_array.append(img)\n",
    "    return img_array\n",
    "\n",
    "training_img = create_batch(TRAIN_DIR)\n",
    "training_img = np.array(training_img)\n",
    "training_img = np.expand_dims(training_img, axis = 3)\n",
    "\n",
    "test_img = create_batch(TEST_DIR)\n",
    "test_img = np.array(test_img)\n",
    "test_img = np.expand_dims(test_img, axis = 3)\n",
    "\n",
    "# Save Processed Images\n",
    "#pickle.dump(training_img[:int(len(training_img)/2)], open('training_img_1.p', 'wb'))\n",
    "#pickle.dump(training_img[int(len(training_img)/2):], open('training_img_2.p', 'wb'))\n",
    "#pickle.dump(test_img, open('pickle_test_img.p', 'wb'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Processed Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with pickle instead of processing images again\n",
    "training_img_1 = pickle.load(open('training_img_1.p', 'rb'))\n",
    "training_img_2 = pickle.load(open('training_img_2.p', 'rb'))\n",
    "training_img = np.append(training_img_1, training_img_2, axis = 0)\n",
    "\n",
    "test_img = pickle.load(open('pickle_test_img.p', 'rb'))\n",
    "\n",
    "training_labels = pickle.load(open('training_labels.p', 'rb'))\n",
    "test_labels = pickle.load(open('test_labels.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_img.shape, \n",
    "     training_labels.shape,\n",
    "     test_img.shape,\n",
    "     test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import models, optimizers, layers\n",
    "from keras.models import model_from_json\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = models.Sequential()\n",
    "baseline.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "baseline.add(layers.MaxPooling2D((2, 2)))\n",
    "baseline.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "baseline.add(layers.MaxPooling2D((2, 2)))\n",
    "baseline.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "baseline.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "baseline.add(layers.Flatten())\n",
    "baseline.add(layers.Dense(64, activation='relu'))\n",
    "baseline.add(layers.Dense(15, activation='sigmoid'))\n",
    "\n",
    "#baseline.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline.compile(optimizer = optimizers.RMSprop(lr = 1e-5), loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_history = baseline.fit(training_img, training_labels, epochs = 40, validation_split = (1 / 9), batch_size = 128, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_history(base_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_base(base_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_test = baseline.evaluate(test_img, test_labels)\n",
    "print(baseline_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = base_history.history['acc']\n",
    "val_acc = base_history.history['val_acc']\n",
    "loss = base_history.history['loss']\n",
    "val_loss = base_history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(15, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = optimizers.RMSprop(lr = 1e-5), loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4C_4MP_1D = model.fit(training_img, training_labels, epochs = 20, validation_split = (1 / 9), batch_size = 128, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = model.evaluate(test_img, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write history object to a file using pickle\n",
    "def save_history(model_name):\n",
    "    hist_str = str(input(\"Save history as: \"))\n",
    "\n",
    "    pickle.dump(model_name.history, open('Training Histories/'+ hist_str + '.p', 'wb'))\n",
    "    \n",
    "    print(\"Saved \" + hist_str + \" to Training Histories folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_history(model_4C_4MP_1D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Results Compared To Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_acc = bases_loaded.history['val_acc']\n",
    "model_acc = model_4C_4MP_1D.history['val_acc']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, base_acc, 'b', label='Base Acc')\n",
    "plt.plot(epochs, model_acc, 'r', label='Model Acc')\n",
    "plt.title('Comparison of Validation Accuracies')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loss = bases_loaded.history['val_loss']\n",
    "model_loss = model_4C_4MP_1D.history['val_loss']\n",
    "plt.plot(epochs, base_loss, 'b', label='Base loss')\n",
    "plt.plot(epochs, model_loss, 'r', label='Model loss')\n",
    "plt.title('Comparison of Validation Losses')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
