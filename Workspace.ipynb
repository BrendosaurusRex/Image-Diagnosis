{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan, linewidth=115)\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import keras\n",
    "from keras import models, optimizers, layers, regularizers\n",
    "from keras.models import model_from_json\n",
    "np.set_printoptions(threshold = np.nan, linewidth = 115)\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config = config)\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "\n",
    "TRAIN_LABEL_DIR = 'Data_Entry_2017.csv'\n",
    "IMG_SIZE = 256\n",
    "\n",
    "# Save Comparison model\n",
    "def save_model(model_name, hist_str, model_str):\n",
    "\n",
    "    pickle.dump(model_name.history, open('Training Histories/'+ hist_str + '.p', 'wb'))\n",
    "    \n",
    "    print(\"Saved \" + hist_str + \" to Training Histories folder\")\n",
    "    \n",
    "    # serialize model to JSON\n",
    "    model_name = model.to_json()\n",
    "    with open(\"CNN Models/\" + model_str + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_name)\n",
    "\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"CNN Models/\" + model_str + \".h5\")\n",
    "    print(\"Saved \" + model_str + \" and weights to CNN Models folder\")\n",
    "\n",
    "    \n",
    "# Load model architecture and weights NOTE: must compile again\n",
    "def load_model():\n",
    "    model_str = str(input(\"Name of model to load: \"))\n",
    "\n",
    "    # load json and create model\n",
    "    json_file = open('CNN Models/' + model_str + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"CNN Models/\" + model_str + \".h5\")\n",
    "    print(\"Loaded \" + model_str + \" and weights from CNN Models folder\")\n",
    "    \n",
    "    return loaded_model\n",
    "    \n",
    "# Load history object\n",
    "def load_history():\n",
    "    hist_str = str(input(\"Name of history to load: \"))\n",
    "\n",
    "    loaded_history = pickle.load(open('Training Histories/' + hist_str + '.p', 'rb'))\n",
    "    \n",
    "    print(\"Loaded \" + hist_str + \" from Training Histories folder\")\n",
    "    \n",
    "    return loaded_history\n",
    "\n",
    "# Metric Analysis\n",
    "def true_accuracy(y_test, predictions):\n",
    "    \n",
    "    pred = np.round_(predictions)\n",
    "    pred = pred.astype(dtype = 'uint8')\n",
    "    \n",
    "    ft = pred == y_test\n",
    "    \n",
    "    acc = []\n",
    "    \n",
    "    counter = 0\n",
    "    while counter < len(ft):\n",
    "        if sum(ft[counter]) < 15:\n",
    "            acc.append(0)\n",
    "            counter += 1\n",
    "        else:\n",
    "            acc.append(1)\n",
    "            counter += 1\n",
    "            \n",
    "    # Accuracy       \n",
    "    Acc = (sum(acc)/len(acc))\n",
    "    \n",
    "    print('\\t Complete Label Accuracy: ', (Acc * 100), '%')\n",
    "    \n",
    "    print('Sum of Fully Correct Predictions: ', sum(acc))\n",
    "    print('\\t\\t    Total Labels: ', len(acc))\n",
    "    \n",
    "    if Acc == 0:\n",
    "        message = 'Feels Devastating (ﾉಥ益ಥ）ﾉ ┻━┻'\n",
    "        \n",
    "    elif Acc > 0 and Acc < 50:\n",
    "        message = 'Feels Awful (੭ ˃̣̣̥ ㅂ˂̣̣̥)੭ु'\n",
    "        \n",
    "    elif Acc >= 50 and Acc < 60:\n",
    "        message = 'Feels Bad (⌯˃̶᷄ ﹏ ˂̶᷄⌯)'\n",
    "        \n",
    "    elif Acc >= 60 and Acc < 70:\n",
    "        message = 'Feels Meh... ┬─┬ノ(ಠ_ಠノ)'\n",
    "    \n",
    "    elif Acc >= 70 and Acc < 80:\n",
    "        message = 'Feels Ok ʕ ·㉨·ʔ'\n",
    "    \n",
    "    elif Acc >= 80 and Acc < 90:\n",
    "        message = 'Feels Better (^._.^)ﾉ'\n",
    "        \n",
    "    elif Acc >= 90 and Acc < 95:\n",
    "        message = 'Feels Hopeful ( •́ ⍨ •̀)'\n",
    "        \n",
    "    elif Acc >= 95 and Acc < 98:\n",
    "        message = 'Feels Good ヽ|･ω･|ゞ'\n",
    "        \n",
    "    elif Acc >= 98:\n",
    "        message = 'Feels Great! ᕙ( * •̀ ᗜ •́ * )ᕗ'\n",
    "        \n",
    "    print('\\n', message)\n",
    "    \n",
    "    return Acc\n",
    "\n",
    "def show_img(array1, array2, index):\n",
    "    plt.imshow(array1[index], cmap = 'gray')\n",
    "    plt.title(\"{}\".format(array2[index]))\n",
    "    plt.show\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Analysis\n",
    "def _1vAll_accuracy(y_test, pred):\n",
    "    \n",
    "    pred = np.squeeze(pred, axis = -1)\n",
    "    pred = np.round_(pred)\n",
    "    pred = pred.astype(dtype = 'uint8')\n",
    "    \n",
    "    ft = pred == y_test\n",
    "    \n",
    "    accuracy = sum(ft)/len(ft)\n",
    "        \n",
    "    print('\\t Complete Label Accuracy: ', (accuracy * 100), '%')\n",
    "    \n",
    "    print('Sum of Fully Correct Predictions: ', sum(ft))\n",
    "    print('\\t\\t    Total Labels: ', len(ft))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with pickle instead of processing images again\n",
    "training_img_1 = pickle.load(open('training_img_resampled_1st_half.p', 'rb'))\n",
    "training_img_2 = pickle.load(open('training_img_resampled_2nd_half.p', 'rb'))\n",
    "training_img = np.append(training_img_1, training_img_2, axis = 0)\n",
    "\n",
    "training_labels_1 = pickle.load(open('training_labels_resampled_1st_half.p', 'rb'))\n",
    "training_labels_2 = pickle.load(open('training_labels_resampled_2nd_half.p', 'rb'))\n",
    "training_labels = np.append(training_labels_1, training_labels_2, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = pickle.load(open('pickle_test_img.p', 'rb'))\n",
    "\n",
    "test_labels = pickle.load(open('test_labels.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: load model architecture\n",
    "model = load_model()\n",
    "model.compile(optimizer = optimizers.RMSprop(lr = 1e-5), loss = 'binary_crossentropy', metrics = ['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "predictions = model.predict(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_preds = np.round_(predictions)\n",
    "show_preds = predictions.astype(dtype='uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(show_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = model.evaluate(test_img, test_labels)\n",
    "print(model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezed_img = np.squeeze(test_img, axis = -1)\n",
    "squeezed_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 11211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(squeezed_img[image_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(show_preds[image_index])\n",
    "show_img(squeezed_img, training_labels, image_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Results Compared To Baseline\n",
    "\n",
    "base_acc = base_hist['val_categorical_accuracy']\n",
    "model_acc = model_history['val_categorical_accuracy']\n",
    "epochs = range(1, len(model_acc) + 1)\n",
    "plt.plot(epochs, base_acc, 'b', label='Dropouts Model Acc')\n",
    "plt.plot(epochs, model_acc, 'r', label='Version 2 Acc')\n",
    "plt.title('Validation Categorical Accuracies')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.show()\n",
    "# Optional: save plot\n",
    "#Todo\n",
    "\n",
    "base_loss = base_hist['val_loss']\n",
    "model_loss = model_history['val_loss']\n",
    "plt.plot(epochs, base_loss, 'b', label='VGG Model loss')\n",
    "plt.plot(epochs, model_loss, 'r', label='Version 2 loss')\n",
    "plt.title('Validation Losses')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "# Optional: save plot\n",
    "#Todo\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Two Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: load model to see summary and/or compile for evaluation\n",
    "loaded_model = load_model()\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.compile(optimizer = optimizers.RMSprop(lr = 1e-5), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "model_test = loaded_model.predict(test_img)\n",
    "print(model_test)\n",
    "\n",
    "#cohen_kappa_score(test_labels, np.round_(loaded_model.predict(test_img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: load history to plot and compare models\n",
    "loaded_hist = load_history()\n",
    "\n",
    "model_history = load_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Validation Accuracies & Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_acc = loaded_hist['val_acc']\n",
    "model_acc = model_history['val_acc']\n",
    "epochs = range(1, len(model_acc) + 1)\n",
    "plt.plot(epochs, loaded_acc, 'b', label='Loaded Model Acc')\n",
    "plt.plot(epochs, model_acc, 'r', label='Trained Model Acc')\n",
    "plt.title('Comparison of Validation Accuracies')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "# Optional: save plot\n",
    "#Todo\n",
    "\n",
    "loaded_loss = loaded_hist['val_loss']\n",
    "model_loss = model_history['val_loss']\n",
    "plt.plot(epochs, loaded_loss, 'b', label='Loaded Model loss')\n",
    "plt.plot(epochs, model_loss, 'r', label='Trained Model loss')\n",
    "plt.title('Comparison of Validation Losses')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "# Optional: save plot\n",
    "#Todo\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]]])\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = arr.reshape((2, 4))\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.reshape(arr, (2, 2, 2))\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
