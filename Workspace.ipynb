{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import keras\n",
    "from keras import models, optimizers, layers, regularizers\n",
    "from keras.models import model_from_json\n",
    "np.set_printoptions(threshold = np.nan, linewidth = 115)\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config = config)\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "\n",
    "IMG_SIZE = 256\n",
    "\n",
    "# Save Comparison model\n",
    "def save_model(model_name, hist_str, model_str):\n",
    "\n",
    "    pickle.dump(model_name.history, open('Training Histories/'+ hist_str + '.p', 'wb'))\n",
    "    \n",
    "    print(\"Saved \" + hist_str + \" to Training Histories folder\")\n",
    "    \n",
    "    # serialize model to JSON\n",
    "    model_name = model.to_json()\n",
    "    with open(\"CNN Models/\" + model_str + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_name)\n",
    "\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"CNN Models/\" + model_str + \".h5\")\n",
    "    print(\"Saved \" + model_str + \" and weights to CNN Models folder\")\n",
    "\n",
    "    \n",
    "# Load model architecture and weights NOTE: must compile again\n",
    "def load_model():\n",
    "    model_str = str(input(\"Name of model to load: \"))\n",
    "\n",
    "    # load json and create model\n",
    "    json_file = open('CNN Models/' + model_str + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"CNN Models/\" + model_str + \".h5\")\n",
    "    print(\"Loaded \" + model_str + \" and weights from CNN Models folder\")\n",
    "    \n",
    "    return loaded_model\n",
    "    \n",
    "# Load history object\n",
    "def load_history():\n",
    "    hist_str = str(input(\"Name of history to load: \"))\n",
    "\n",
    "    loaded_history = pickle.load(open('Training Histories/' + hist_str + '.p', 'rb'))\n",
    "    \n",
    "    print(\"Loaded \" + hist_str + \" from Training Histories folder\")\n",
    "    \n",
    "    return loaded_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with pickle instead of processing images again\n",
    "training_img_1 = pickle.load(open('training_img_1.p', 'rb'))\n",
    "training_img_2 = pickle.load(open('training_img_2.p', 'rb'))\n",
    "training_img = np.append(training_img_1, training_img_2, axis = 0)\n",
    "\n",
    "test_img = pickle.load(open('pickle_test_img.p', 'rb'))\n",
    "\n",
    "training_labels = pickle.load(open('training_labels.p', 'rb'))\n",
    "test_labels = pickle.load(open('test_labels.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: load model architecture\n",
    "model = load_model()\n",
    "model.compile(optimizer = optimizers.RMSprop(lr = 1e-5), loss = 'binary_crossentropy', metrics = ['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "pred = model.predict(test_img)\n",
    "\n",
    "pred = pred.astype(dtype = 'int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Analysis\n",
    "def true_accuracy(pred, y_test):\n",
    "    ft = pred == y_test\n",
    "    \n",
    "    acc = []\n",
    "    \n",
    "    counter = 0\n",
    "    while counter < len(ft):\n",
    "        if sum(ft[counter]) < 15:\n",
    "            acc.append(0)\n",
    "            counter += 1\n",
    "        else:\n",
    "            acc.append(1)\n",
    "            counter += 1\n",
    "            \n",
    "    # Accuracy       \n",
    "    Acc = (sum(acc)/len(acc))\n",
    "    \n",
    "    print('\\t Complete Label Accuracy: ', (Acc * 100), '%')\n",
    "    \n",
    "    print('Sum of Fully Correct Predictions: ', sum(acc))\n",
    "    print('\\t\\t    Total Labels: ', len(acc))\n",
    "    \n",
    "    if Acc == 0:\n",
    "        message = 'Feels Devastating (ﾉಥ益ಥ）ﾉ ┻━┻'\n",
    "        \n",
    "    elif Acc > 0 and Acc < 50:\n",
    "        message = 'Feels Awful (੭ ˃̣̣̥ ㅂ˂̣̣̥)੭ु'\n",
    "        \n",
    "    elif Acc >= 50 and Acc < 60:\n",
    "        message = 'Feels Bad (⌯˃̶᷄ ﹏ ˂̶᷄⌯)'\n",
    "        \n",
    "    elif Acc >= 60 and Acc < 70:\n",
    "        message = 'Feels Meh... ┬─┬ノ(ಠ_ಠノ)'\n",
    "    \n",
    "    elif Acc >= 70 and Acc < 80:\n",
    "        message = 'Feels Ok ʕ ·㉨·ʔ'\n",
    "    \n",
    "    elif Acc >= 80 and Acc < 90:\n",
    "        message = 'Feels Better (^._.^)ﾉ'\n",
    "        \n",
    "    elif Acc >= 90 and Acc < 95:\n",
    "        message = 'Feels Hopeful ( •́ ⍨ •̀)'\n",
    "        \n",
    "    elif Acc >= 95 and Acc < 98:\n",
    "        message = 'Feels Good ヽ|･ω･|ゞ'\n",
    "        \n",
    "    elif Acc >= 98:\n",
    "        message = 'Feels Great! ᕙ( * •̀ ᗜ •́ * )ᕗ'\n",
    "        \n",
    "    print('\\n', message)\n",
    "    \n",
    "    return Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = true_accuracy(pred, test_labels)\n",
    "print('accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = model.evaluate(test_img, test_labels)\n",
    "print(model_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Save History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline history\n",
    "base_hist = load_history()\n",
    "\n",
    "model_history = load_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Results Compared To Baseline\n",
    "\n",
    "base_acc = base_hist['val_categorical_accuracy']\n",
    "model_acc = model_history['val_categorical_accuracy']\n",
    "epochs = range(1, len(model_acc) + 1)\n",
    "plt.plot(epochs, base_acc, 'b', label='Dropouts Model Acc')\n",
    "plt.plot(epochs, model_acc, 'r', label='Version 2 Acc')\n",
    "plt.title('Validation Categorical Accuracies')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.show()\n",
    "# Optional: save plot\n",
    "#Todo\n",
    "\n",
    "base_loss = base_hist['val_loss']\n",
    "model_loss = model_history['val_loss']\n",
    "plt.plot(epochs, base_loss, 'b', label='VGG Model loss')\n",
    "plt.plot(epochs, model_loss, 'r', label='Version 2 loss')\n",
    "plt.title('Validation Losses')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "# Optional: save plot\n",
    "#Todo\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Two Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: load model to see summary and/or compile for evaluation\n",
    "loaded_model = load_model()\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.compile(optimizer = optimizers.RMSprop(lr = 1e-5), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "model_test = loaded_model.predict(test_img)\n",
    "print(model_test)\n",
    "\n",
    "#cohen_kappa_score(test_labels, np.round_(loaded_model.predict(test_img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: load history to plot and compare models\n",
    "loaded_hist = load_history()\n",
    "\n",
    "model_history = load_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Validation Accuracies & Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_acc = loaded_hist['val_acc']\n",
    "model_acc = model_history['val_acc']\n",
    "epochs = range(1, len(model_acc) + 1)\n",
    "plt.plot(epochs, loaded_acc, 'b', label='Loaded Model Acc')\n",
    "plt.plot(epochs, model_acc, 'r', label='Trained Model Acc')\n",
    "plt.title('Comparison of Validation Accuracies')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "# Optional: save plot\n",
    "#Todo\n",
    "\n",
    "loaded_loss = loaded_hist['val_loss']\n",
    "model_loss = model_history['val_loss']\n",
    "plt.plot(epochs, loaded_loss, 'b', label='Loaded Model loss')\n",
    "plt.plot(epochs, model_loss, 'r', label='Trained Model loss')\n",
    "plt.title('Comparison of Validation Losses')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "# Optional: save plot\n",
    "#Todo\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 1],\n",
       "        [2, 3]],\n",
       "\n",
       "       [[4, 5],\n",
       "        [6, 7]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]]])\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3],\n",
       "       [4, 5, 6, 7]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = arr.reshape((2, 4))\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 1],\n",
       "        [2, 3]],\n",
       "\n",
       "       [[4, 5],\n",
       "        [6, 7]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.reshape(arr, (2, 2, 2))\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
